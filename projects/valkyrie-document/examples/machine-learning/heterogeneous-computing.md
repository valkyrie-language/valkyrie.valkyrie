# å¼‚æ„è®¡ç®—

åœ¨ Valkyrie ä¸­ï¼Œå¼‚æ„è®¡ç®—é€šè¿‡ç»Ÿä¸€çš„æ•°ç»„æŠ½è±¡å®ç°è·¨è®¾å¤‡çš„é«˜æ€§èƒ½è®¡ç®—ï¼Œä¸“æ³¨äºä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•çš„åŠ é€Ÿã€‚

## æ•°ç»„ç±»å‹ä½“ç³»

- `Array` æ˜¯ä¸€æ®µå†…å­˜ä¸­çš„æ•°æ®
- `Array<T, N>` æ˜¯ `[T; N]` çš„è¯­æ³•ç³–ï¼Œè¡¨ç¤ºå›ºå®šå¤§å°æ•°ç»„
- `ArrayND` æ˜¯å¼‚æ„è®¡ç®—ã€æœºå™¨å­¦ä¹ çš„åŸºç¡€å¤šç»´æ•°ç»„ç±»å‹
- `Array1D` æ˜¯ `ArrayND` çš„ type aliasï¼Œä¸“é—¨ç”¨äºä¸€ç»´æ•°ç»„æ“ä½œ

ArrayND å¯ä»¥é€‰æ‹©ä¸åŒçš„è®¾å¤‡ (device) å’Œå¸ƒå±€ (layout)ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¿›è¡Œä¼˜åŒ–ã€‚

## åŸºæœ¬ç”¨æ³•

### åˆ›å»ºæ•°ç»„

```valkyrie
# åˆ›å»ºä¸åŒç»´åº¦çš„æ•°ç»„
let vector = Array1D::zeros(1000)           # ä¸€ç»´æ•°ç»„
let matrix = ArrayND::zeros([1000, 1000])   # äºŒç»´æ•°ç»„
let tensor = ArrayND::zeros([32, 3, 224, 224])  # å››ç»´å¼ é‡

# ä»æ•°æ®åˆ›å»º
let data = [1.0, 2.0, 3.0, 4.0]
let arr = Array1D::from(data)
let reshaped = arr.reshape([2, 2])  # é‡å¡‘ä¸º2x2çŸ©é˜µ
```

### è®¾å¤‡é€‰æ‹©

```valkyrie
# åœ¨ä¸åŒè®¾å¤‡ä¸Šåˆ›å»ºæ•°ç»„
let cpu_array = ArrayND::zeros([1024, 1024]).on_device(Device::CPU)
let gpu_array = ArrayND::zeros([1024, 1024]).on_device(Device::GPU)

# è®¾å¤‡é—´è½¬æ¢
let gpu_result = cpu_array.to_device(Device::GPU)
let cpu_result = gpu_array.to_cpu()
```

## å¸ƒå±€ä¼˜åŒ–

### è‡ªåŠ¨å¸ƒå±€é€‰æ‹©

```valkyrie
# ç³»ç»Ÿä¼šæ ¹æ®æ“ä½œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¸ƒå±€
let a = ArrayND::random([1024, 1024])
let b = ArrayND::random([1024, 1024])

# çŸ©é˜µä¹˜æ³•ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„å†…å­˜å¸ƒå±€
let result = a.matmul(&b)  # è‡ªåŠ¨ä¼˜åŒ–

# æ‰‹åŠ¨æŒ‡å®šå¸ƒå±€ï¼ˆé€šå¸¸ä¸éœ€è¦ï¼‰
let row_major = a.with_layout(Layout::RowMajor)
let col_major = a.with_layout(Layout::ColMajor)
```

### å¼‚æ­¥æ•°æ®ä¼ è¾“

```valkyrie
# å¼‚æ­¥è®¾å¤‡é—´ä¼ è¾“
async micro process_large_data() {
    let data = ArrayND::load("dataset.bin")
    let gpu_data = data.to_device_async(Device::GPU).await
    
    # GPUè®¡ç®—
    let result = gpu_data.matmul(&gpu_data.transpose())
    
    # ä¼ å›CPUä¿å­˜
    let cpu_result = result.to_cpu_async().await
    cpu_result.save("result.bin")
}
```

## å¸¸ç”¨æ“ä½œ

### æ•°å­¦è¿ç®—

```valkyrie
# åŸºæœ¬è¿ç®—ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡å’Œå¸ƒå±€ï¼‰
let ğ€ = ArrayND::random([1000, 1000])
let ğ = ArrayND::random([1000, 1000])

# çŸ©é˜µè¿ç®—
let sum = ğ€ + ğ
let product = ğ€ âŠ™ ğ  # é€å…ƒç´ ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰
let matmul = ğ€ Â· ğ     # çŸ©é˜µä¹˜æ³•
let transpose = ğ€áµ€

# ç»Ÿè®¡è¿ç®—
let Î¼ = ğ€.mean()      # å‡å€¼
let Î£ = ğ€.sum()       # æ±‚å’Œ
let max_val = max(ğ€)  # æœ€å¤§å€¼
let Ïƒ = ğ€.std()       # æ ‡å‡†å·®
```

## æœºå™¨å­¦ä¹ åº”ç”¨

### çº¿æ€§å›å½’

```valkyrie
# çº¿æ€§å›å½’æ¨¡å‹
class LinearRegression {
    weights: ArrayND,
    bias: f32,
}

imply LinearRegression {
    new(n_features: usize) -> Self {
        Self {
            weights: ArrayND::zeros([n_features]),
            bias: 0.0,
        }
    }
    
    fit(&mut self, ğ—: &ArrayND, ğ²: &ArrayND, Î±: f32, epochs: usize) {
        let n = ğ—.shape()[0] as f32  # æ ·æœ¬æ•°é‡
        
        for _ in 0..epochs {
            # é¢„æµ‹
            let Å· = ğ— Â· &self.weights + self.bias
            
            # è®¡ç®—æ¢¯åº¦
            let Îµ = Å· - ğ²  # è¯¯å·®
            let âˆ‡w = ğ—áµ€ Â· &Îµ / n  # æƒé‡æ¢¯åº¦
            let âˆ‡b = Îµ.mean()      # åç½®æ¢¯åº¦
            
            # æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
            self.weights = &self.weights - Î± * &âˆ‡w
            self.bias -= Î± * âˆ‡b
        }
    }
    
    predict(&self, ğ—: &ArrayND) -> ArrayND {
        ğ— Â· &self.weights + self.bias
    }
}
```

### æ”¯æŒå‘é‡æœº (SVM)

```valkyrie
# SVM åˆ†ç±»å™¨
class SVM {
    weights: ArrayND,
    bias: f32,
    C: f32,  # æ­£åˆ™åŒ–å‚æ•°
}

imply SVM {
    new(n_features: usize, C: f32) -> Self {
        Self {
            weights: ArrayND::zeros([n_features]),
            bias: 0.0,
            C,
        }
    }
    
    fit(&mut self, ğ—: &ArrayND, ğ²: &ArrayND, Î±: f32, epochs: usize) {
        for _ in 0..epochs {
            for i in 0..ğ—.shape()[0] {
                let ğ±áµ¢ = ğ—.row(i)
                let yáµ¢ = ğ²[i]
                
                let decision = ğ±áµ¢ Â· &self.weights + self.bias
                
                if yáµ¢ * decision < 1.0 {
                    # æ”¯æŒå‘é‡ï¼Œæ›´æ–°å‚æ•°
                    self.weights = &self.weights + Î± * (yáµ¢ * &ğ±áµ¢ - 2.0 * self.C * &self.weights)
                    self.bias += Î± * yáµ¢
                } else {
                    # æ­£ç¡®åˆ†ç±»ï¼Œåªåº”ç”¨æ­£åˆ™åŒ–
                    self.weights = &self.weights - Î± * 2.0 * self.C * &self.weights
                }
            }
        }
    }
    
    predict(&self, ğ—: &ArrayND) -> ArrayND {
        sign(ğ— Â· &self.weights + self.bias)
    }
}
```

## å¤šè®¾å¤‡å¹¶è¡Œ

### K-Means èšç±»å¹¶è¡ŒåŒ–

```valkyrie
# å¹¶è¡Œ K-Means èšç±»
class ParallelKMeans {
    centroids: ArrayND,
    k: usize,
    devices: Vector<Device>,
}

imply ParallelKMeans {
    fit(&mut self, data: &ArrayND, max_iters: usize) {
        let n_devices = self.devices.len()
        let chunk_size = data.shape()[0] / n_devices
        
        for _ in 0..max_iters {
            # å¹¶è¡Œè®¡ç®—è·ç¦»å’Œåˆ†é…
            let assignments: Vector<ArrayND> = data.chunks(chunk_size)
                .zip(&self.devices)
                .par_iter()
                .map(|(chunk, device)| {
                    let chunk_gpu = chunk.to_device(device)
                    let centroids_gpu = self.centroids.to_device(device)
                    
                    # è®¡ç®—è·ç¦»çŸ©é˜µ
                    let distances = chunk_gpu.cdist(&centroids_gpu)
                    distances.argmin(1)  # æœ€è¿‘è´¨å¿ƒç´¢å¼•
                })
                .collect()
            
            # æ›´æ–°è´¨å¿ƒ
            self.update_centroids(data, &assignments)
        }
    }
    
    update_centroids(&mut self, data: &ArrayND, assignments: &[ArrayND]) {
        for k in 0..self.k {
            let mask = assignments.iter()
                .map(|assign| assign.eq(k))
                .reduce(|acc, x| acc.concat(&x))
                .unwrap()
            
            let cluster_points = data.masked_select(&mask)
            if cluster_points.shape()[0] > 0 {
                self.centroids.row_mut(k).copy_from(&cluster_points.mean(0))
            }
        }
    }
}
```

### éšæœºæ£®æ—å¹¶è¡ŒåŒ–

```valkyrie
# å¹¶è¡Œéšæœºæ£®æ—
class ParallelRandomForest {
    trees: Vector<DecisionTree>,
    n_trees: usize,
}

imply ParallelRandomForest {
    fit(&mut self, X: &ArrayND, y: &ArrayND) {
        # å¹¶è¡Œè®­ç»ƒå†³ç­–æ ‘
        self.trees = (0..self.n_trees)
            .into_par_iter()
            .map(|_| {
                # è‡ªåŠ©é‡‡æ ·
                let (X_sample, y_sample) = bootstrap_sample(X, y)
                
                # è®­ç»ƒå•æ£µæ ‘
                let mut tree = DecisionTree::new()
                tree.fit(&X_sample, &y_sample)
                tree
            })
            .collect()
    }
    
    predict(&self, X: &ArrayND) -> ArrayND {
        # å¹¶è¡Œé¢„æµ‹
        let predictions: Vector<ArrayND> = self.trees
            .par_iter()
            .map(|tree| tree.predict(X))
            .collect()
        
        # æŠ•ç¥¨èšåˆ
        majority_vote(&predictions)
    }
}
```

## å†…å­˜ä¼˜åŒ–ç­–ç•¥

### æ•°æ®åˆ†å—å¤„ç†

```valkyrie
# å¤§æ•°æ®é›†åˆ†å—å¤„ç†
class ChunkedProcessor {
    chunk_size: usize,
    memory_limit: usize,
}

imply ChunkedProcessor {
    process_large_dataset(&self, data: &ArrayND, algorithm: &dyn Algorithm) -> ArrayND {
        let total_size = data.shape()[0]
        let mut results = Vec::new()
        
        for start in (0..total_size).step_by(self.chunk_size) {
            let end = (start + self.chunk_size).min(total_size)
            let chunk = data.slice([start..end, ..])
            
            # å¤„ç†å•ä¸ªæ•°æ®å—
            let chunk_result = algorithm.process(&chunk)
            results.push(chunk_result)
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            if self.get_memory_usage() > self.memory_limit {
                self.gc_collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            }
        }
        
        # åˆå¹¶ç»“æœ
        ArrayND::concat(&results, 0)
    }
    
    get_memory_usage(&self) -> usize {
        # è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡
        std::mem::size_of_val(self) + self.estimate_array_memory()
    }
}
```

### å†…å­˜ä½¿ç”¨æ£€æŸ¥

```valkyrie
# ç®€å•çš„å†…å­˜ä½¿ç”¨ç›‘æ§
micro check_memory_usage() {
    let cpu_arrays = ArrayND::get_cpu_memory_usage()
    let gpu_arrays = ArrayND::get_gpu_memory_usage()
    
    println!("CPU å†…å­˜ä½¿ç”¨: {:.2} MB", cpu_arrays as f64 / 1024.0 / 1024.0)
    println!("GPU å†…å­˜ä½¿ç”¨: {:.2} MB", gpu_arrays as f64 / 1024.0 / 1024.0)
    
    # å†…å­˜ä¸è¶³æ—¶çš„å¤„ç†
    if gpu_arrays > 8 * 1024 * 1024 * 1024 {  # 8GB
        println!("GPU å†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨ CPU æˆ–å‡å°‘æ‰¹æ¬¡å¤§å°")
    }
}

# è‡ªåŠ¨å†…å­˜æ¸…ç†
micro auto_cleanup() {
    # æ¸…ç†æœªä½¿ç”¨çš„æ•°ç»„
    ArrayND::gc_collect()
    
    # é‡Šæ”¾ä¸´æ—¶ç¼“å­˜
    ArrayND::clear_cache()
}
```

## æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜

### ç®€å•æ€§èƒ½æµ‹è¯•

```valkyrie
# æµ‹è¯•ç®—æ³•æ€§èƒ½
micro benchmark_algorithm<F>(name: &str, f: F) -> Duration 
where F: FnOnce() {
    let start = std::time::Instant::now()
    f()
    let duration = start.elapsed()
    
    println!("{} è€—æ—¶: {:.2}ms", name, duration.as_millis())
    duration
}

# æ¯”è¾ƒä¸åŒè®¾å¤‡æ€§èƒ½
micro compare_devices() {
    let data = ArrayND::random([1000, 1000])
    
    # CPU è®¡ç®—
    let cpu_time = benchmark_algorithm("CPU çŸ©é˜µä¹˜æ³•", || {
        let result = data.matmul(&data)
    })
    
    # GPU è®¡ç®—
    let gpu_time = benchmark_algorithm("GPU çŸ©é˜µä¹˜æ³•", || {
        let data_gpu = data.to_device(&Device::GPU(0))
        let result = data_gpu.matmul(&data_gpu)
    })
    
    println!("GPU åŠ é€Ÿæ¯”: {:.2}x", cpu_time.as_millis() as f64 / gpu_time.as_millis() as f64)
}```
