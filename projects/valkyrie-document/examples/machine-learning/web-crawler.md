# ç½‘ç»œçˆ¬è™«

Valkyrie æä¾›äº†å¼ºå¤§çš„ç½‘ç»œçˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒå¼‚æ­¥å¹¶å‘ã€æ™ºèƒ½è°ƒåº¦ã€æ•°æ®æå–å’Œå­˜å‚¨ç­‰åŠŸèƒ½ã€‚é€šè¿‡ç±»å‹å®‰å…¨çš„ API å’Œé›¶æˆæœ¬æŠ½è±¡ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®é‡‡é›†æä¾›é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚

## æ ¸å¿ƒç‰¹æ€§

### å¼‚æ­¥ HTTP å®¢æˆ·ç«¯

```valkyrie
use std::http::{HttpClient, Request, Response, Method}
use std::async::{Future, Stream}
use std::time::Duration

# HTTP å®¢æˆ·ç«¯é…ç½®
struct CrawlerConfig {
    max_concurrent: usize,      # æœ€å¤§å¹¶å‘æ•° âˆ¥
    request_delay: Duration,    # è¯·æ±‚é—´éš” Î”t
    timeout: Duration,          # è¶…æ—¶æ—¶é—´ Ï„
    user_agent: String,         # ç”¨æˆ·ä»£ç†
    max_retries: u32           # æœ€å¤§é‡è¯•æ¬¡æ•° â„
}

# å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
class AsyncHttpClient {
    config: CrawlerConfig,
    client: HttpClient
    
    async micro new(config: CrawlerConfig) -> AsyncHttpClient {
        let client = HttpClient::builder()
            .timeout(config.timeout)
            .user_agent(config.user_agent.clone())
            .build()
            
        AsyncHttpClient { config, client }
    }
    
    async micro get(self, url: &str) -> Result<Response, CrawlerError> {
        let request = Request::builder()
            .method(Method::GET)
            .uri(url)
            .build()?
            
        let mut â„ = 0  # é‡è¯•è®¡æ•°å™¨
        
        loop {
            match self.client.send(request.clone()).await {
                Ok(response) => {
                    if response.status().is_success() {
                        return Ok(response)  # æˆåŠŸå“åº” âœ“
                    } else if response.status().is_server_error() && â„ < self.config.max_retries {
                        â„ += 1
                        tokio::time::sleep(Duration::from_secs(2_u64.pow(â„))).await  # æŒ‡æ•°é€€é¿ 2^â„
                        continue
                    } else {
                        return Err(CrawlerError::HttpError(response.status()))  # HTTPé”™è¯¯ âœ—
                    }
                },
                Err(e) if â„ < self.config.max_retries => {
                    â„ += 1
                    tokio::time::sleep(Duration::from_secs(2_u64.pow(â„))).await  # æŒ‡æ•°é€€é¿ 2^â„
                    continue
                },
                Err(e) => return Err(CrawlerError::NetworkError(e))  # ç½‘ç»œé”™è¯¯ âœ—
            }
        }
    }
    
    async micro post(self, url: &str, body: &[u8]) -> Result<Response, CrawlerError> {
        let request = Request::builder()
            .method(Method::POST)
            .uri(url)
            .header("Content-Type", "application/json")
            .body(body.to_vec())
            .build()?
            
        self.send_with_retry(request).await
    }
}

enum CrawlerError {
    HttpError(StatusCode),
    NetworkError(HttpError),
    ParseError(String),
    RateLimitExceeded,
    Timeout
}
```

### URL ç®¡ç†å’Œè°ƒåº¦

```valkyrie
use std::collections::{HashSet, VecDeque}
use std::sync::{Arc, Mutex}
use std::hash::{Hash, Hasher}

# URL ä¼˜å…ˆçº§é˜Ÿåˆ—
struct UrlQueue {
    high_priority: VecDeque<CrawlTask>,
    normal_priority: VecDeque<CrawlTask>,
    low_priority: VecDeque<CrawlTask>,
    visited: HashSet<String>,
    in_progress: HashSet<String>
}

struct CrawlTask {
    url: String,
    depth: u32,
    priority: Priority,
    metadata: TaskMetadata
}

enum Priority {
    High = 3,
    Normal = 2,
    Low = 1
}

struct TaskMetadata {
    referrer: Option<String>,
    created_at: SystemTime,
    retry_count: u32,
    custom_headers: HashMap<String, String>
}

impl UrlQueue {
    micro new() -> UrlQueue {
        UrlQueue {
            high_priority: VecDeque::new(),
            normal_priority: VecDeque::new(),
            low_priority: VecDeque::new(),
            visited: HashSet::new(),
            in_progress: HashSet::new()
        }
    }
    
    micro add_url(mut self, url: String, priority: Priority, depth: u32) -> bool {
        if self.visited.contains(&url) || self.in_progress.contains(&url) {
            return false  # URLå·²å­˜åœ¨ âˆƒ
        }
        
        let task = CrawlTask {
            url: url.clone(),
            depth,  # çˆ¬å–æ·±åº¦ Î´
            priority,  # ä¼˜å…ˆçº§ Ï€
            metadata: TaskMetadata {
                referrer: None,
                created_at: SystemTime::now(),  # åˆ›å»ºæ—¶é—´ tâ‚€
                retry_count: 0,  # é‡è¯•æ¬¡æ•° â„
                custom_headers: HashMap::new()
            }
        }
        
        match priority {
            Priority::High => self.high_priority.push_back(task),    # é«˜ä¼˜å…ˆçº§é˜Ÿåˆ— Qâ‚ƒ
            Priority::Normal => self.normal_priority.push_back(task), # æ™®é€šä¼˜å…ˆçº§é˜Ÿåˆ— Qâ‚‚
            Priority::Low => self.low_priority.push_back(task)       # ä½ä¼˜å…ˆçº§é˜Ÿåˆ— Qâ‚
        }
        
        true  # æ·»åŠ æˆåŠŸ âœ“
    }
    
    micro next_task(mut self) -> Option<CrawlTask> {
        if let Some(task) = self.high_priority.pop_front() {
            self.in_progress.insert(task.url.clone())
            return Some(task)
        }
        
        if let Some(task) = self.normal_priority.pop_front() {
            self.in_progress.insert(task.url.clone())
            return Some(task)
        }
        
        if let Some(task) = self.low_priority.pop_front() {
            self.in_progress.insert(task.url.clone())
            return Some(task)
        }
        
        None
    }
    
    micro mark_completed(mut self, url: &str) {
        self.in_progress.remove(url)
        self.visited.insert(url.to_string())
    }
    
    micro mark_failed(mut self, url: &str) {
        self.in_progress.remove(url)
    }
    
    micro is_empty(self) -> bool {
        self.high_priority.is_empty() && 
        self.normal_priority.is_empty() && 
        self.low_priority.is_empty()
    }
}

# æ™ºèƒ½è°ƒåº¦å™¨
class CrawlerScheduler {
    queue: Arc<Mutex<UrlQueue>>,
    rate_limiter: RateLimiter,
    domain_delays: HashMap<String, SystemTime>
    
    micro new(requests_per_second: f64) -> CrawlerScheduler {
        CrawlerScheduler {
            queue: Arc::new(Mutex::new(UrlQueue::new())),
            rate_limiter: RateLimiter::new(requests_per_second),
            domain_delays: HashMap::new()
        }
    }
    
    async micro schedule_request(mut self) -> Option<CrawlTask> {
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait().await
        
        let mut queue = self.queue.lock().unwrap()
        
        # å¯»æ‰¾å¯ä»¥ç«‹å³å¤„ç†çš„ä»»åŠ¡ âˆƒ
        while let Some(task) = queue.next_task() {
            let ğ’Ÿ = extract_domain(&task.url)?  # æå–åŸŸå
            
            # æ£€æŸ¥åŸŸåå»¶è¿Ÿ Î”t
            if let Some(t_last) = self.domain_delays.get(&ğ’Ÿ) {
                let Î”t_elapsed = SystemTime::now().duration_since(*t_last).unwrap_or_default()
                if Î”t_elapsed < Duration::from_secs(1) {
                    # é‡æ–°åŠ å…¥é˜Ÿåˆ—ï¼Œé™ä½ä¼˜å…ˆçº§ Ï€â†“
                    queue.add_url(task.url, Priority::Low, task.depth)
                    continue
                }
            }
            
            self.domain_delays.insert(ğ’Ÿ, SystemTime::now())  # æ›´æ–°æ—¶é—´æˆ³ t_now
            return Some(task)  # è¿”å›ä»»åŠ¡ âœ“
        }
        
        None
    }
}

# é€Ÿç‡é™åˆ¶å™¨
struct RateLimiter {
    interval: Duration,
    last_request: Option<SystemTime>
}

impl RateLimiter {
    micro new(requests_per_second: f64) -> RateLimiter {
        RateLimiter {
            interval: Duration::from_secs_f64(1.0 / requests_per_second),
            last_request: None
        }
    }
    
    async micro wait(mut self) {
        if let Some(t_last) = self.last_request {  # ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
            let Î”t_elapsed = SystemTime::now().duration_since(t_last).unwrap_or_default()  # å·²è¿‡æ—¶é—´
            if Î”t_elapsed < self.interval {
                let Î”t_sleep = self.interval - Î”t_elapsed  # éœ€è¦ç­‰å¾…çš„æ—¶é—´
                tokio::time::sleep(Î”t_sleep).await  # ä¼‘çœ  â³
            }
        }
        
        self.last_request = Some(SystemTime::now())  # æ›´æ–°æ—¶é—´æˆ³ t_now
    }
}
```

### HTML è§£æå’Œæ•°æ®æå–

```valkyrie
use std::html::{Document, Element, Selector}
use std::regex::Regex

# HTML è§£æå™¨
struct HtmlParser {
    base_url: String
}

impl HtmlParser {
    micro new(base_url: String) -> HtmlParser {
        HtmlParser { base_url }
    }
    
    micro parse(self, html: &str) -> Result<Document, ParseError> {
        Document::from_html(html)
            .map_err(|e| ParseError::HtmlParseError(e.to_string()))
    }
    
    micro extract_links(self, doc: &Document) -> Vec<String> {
        let ğ’® = Selector::parse("a[href]").unwrap()  # é“¾æ¥é€‰æ‹©å™¨
        let mut ğ•ƒ = Vec::new()  # é“¾æ¥é›†åˆ
        
        for element in doc.select(&ğ’®) {
            if let Some(href) = element.value().attr("href") {
                if let Some(absolute_url) = self.resolve_url(href) {
                    ğ•ƒ.push(absolute_url)  # æ·»åŠ åˆ°é“¾æ¥é›†åˆ âˆˆ
                }
            }
        }
        
        ğ•ƒ  # è¿”å›é“¾æ¥é›†åˆ
    }
    
    micro extract_text(self, doc: &Document, selector: &str) -> Vec<String> {
        let css_selector = Selector::parse(selector).unwrap()
        let mut texts = Vec::new()
        
        for element in doc.select(&css_selector) {
            let text = element.text().collect::<Vec<_>>().join(" ").trim().to_string()
            if !text.is_empty() {
                texts.push(text)
            }
        }
        
        texts
    }
    
    micro extract_structured_data(self, doc: &Document) -> StructuredData {
        let mut data = StructuredData::new()
        
        # æå–æ ‡é¢˜
        if let Some(title) = doc.select(&Selector::parse("title").unwrap()).next() {
            data.title = Some(title.text().collect::<String>())
        }
        
        # æå–å…ƒæ•°æ®
        for meta in doc.select(&Selector::parse("meta").unwrap()) {
            if let (Some(name), Some(content)) = (meta.value().attr("name"), meta.value().attr("content")) {
                data.meta.insert(name.to_string(), content.to_string())
            }
        }
        
        # æå– JSON-LD ç»“æ„åŒ–æ•°æ®
        for script in doc.select(&Selector::parse("script[type='application/ld+json']").unwrap()) {
            let json_text = script.text().collect::<String>()
            if let Ok(json_data) = serde_json::from_str::<serde_json::Value>(&json_text) {
                data.json_ld.push(json_data)
            }
        }
        
        data
    }
    
    micro resolve_url(self, href: &str) -> Option<String> {
        if href.starts_with("http://") || href.starts_with("https://") {
            Some(href.to_string())
        } else if href.starts_with("/") {
            Some(format!("{}{}", self.base_url, href))
        } else if href.starts_with("./") {
            Some(format!("{}/{}", self.base_url, &href[2..]))
        } else if !href.starts_with("#") && !href.starts_with("mailto:") && !href.starts_with("javascript:") {
            Some(format!("{}/{}", self.base_url, href))
        } else {
            None
        }
    }
}

struct StructuredData {
    title: Option<String>,
    meta: HashMap<String, String>,
    json_ld: Vec<serde_json::Value>,
    custom_fields: HashMap<String, String>
}

impl StructuredData {
    micro new() -> StructuredData {
        StructuredData {
            title: None,
            meta: HashMap::new(),
            json_ld: Vec::new(),
            custom_fields: HashMap::new()
        }
    }
}

enum ParseError {
    HtmlParseError(String),
    SelectorError(String),
    JsonParseError(String)
}
```

### æ•°æ®å­˜å‚¨å’Œç®¡é“

```valkyrie
use std::database::{Database, Connection, Transaction}
use std::fs::{File, OpenOptions}
use std::io::{Write, BufWriter}

# æ•°æ®å­˜å‚¨æ¥å£
trait DataStore {
    async micro save_page(mut self, url: &str, data: &CrawledData) -> Result<(), StoreError>
    async micro save_links(mut self, parent_url: &str, links: &[String]) -> Result<(), StoreError>
    async micro get_crawled_urls(self) -> Result<HashSet<String>, StoreError>
}

# çˆ¬å–çš„æ•°æ®ç»“æ„
struct CrawledData {
    url: String,
    title: Option<String>,
    content: String,
    links: Vec<String>,
    metadata: StructuredData,
    crawled_at: SystemTime,
    response_time: Duration,
    status_code: u16
}

# æ•°æ®åº“å­˜å‚¨å®ç°
struct DatabaseStore {
    connection: Connection
}

impl DataStore for DatabaseStore {
    async micro save_page(mut self, url: &str, data: &CrawledData) -> Result<(), StoreError> {
        let mut tx = self.connection.begin().await?
        
        # ä¿å­˜é¡µé¢æ•°æ®
        sqlx::query!(
            "INSERT INTO pages (url, title, content, status_code, crawled_at, response_time) 
             VALUES ($1, $2, $3, $4, $5, $6) 
             ON CONFLICT (url) DO UPDATE SET 
             title = EXCLUDED.title, 
             content = EXCLUDED.content, 
             status_code = EXCLUDED.status_code, 
             crawled_at = EXCLUDED.crawled_at, 
             response_time = EXCLUDED.response_time",
            data.url,
            data.title,
            data.content,
            data.status_code as i32,
            data.crawled_at,
            data.response_time.as_millis() as i64
        ).execute(&mut tx).await?
        
        # ä¿å­˜å…ƒæ•°æ®
        for (key, value) in &data.metadata.meta {
            sqlx::query!(
                "INSERT INTO page_metadata (url, key, value) VALUES ($1, $2, $3) 
                 ON CONFLICT (url, key) DO UPDATE SET value = EXCLUDED.value",
                data.url, key, value
            ).execute(&mut tx).await?
        }
        
        tx.commit().await?
        Ok(())
    }
    
    async micro save_links(mut self, parent_url: &str, links: &[String]) -> Result<(), StoreError> {
        let mut tx = self.connection.begin().await?
        
        for link in links {
            sqlx::query!(
                "INSERT INTO links (parent_url, target_url, discovered_at) 
                 VALUES ($1, $2, $3) 
                 ON CONFLICT (parent_url, target_url) DO NOTHING",
                parent_url, link, SystemTime::now()
            ).execute(&mut tx).await?
        }
        
        tx.commit().await?
        Ok(())
    }
    
    async micro get_crawled_urls(self) -> Result<HashSet<String>, StoreError> {
        let rows = sqlx::query!("SELECT url FROM pages")
            .fetch_all(&self.connection).await?
        
        Ok(rows.into_iter().map(|row| row.url).collect())
    }
}

# JSON æ–‡ä»¶å­˜å‚¨å®ç°
struct JsonFileStore {
    file_path: String,
    writer: BufWriter<File>
}

impl JsonFileStore {
    micro new(file_path: String) -> Result<JsonFileStore, StoreError> {
        let file = OpenOptions::new()
            .create(true)
            .append(true)
            .open(&file_path)?
        
        Ok(JsonFileStore {
            file_path,
            writer: BufWriter::new(file)
        })
    }
}

impl DataStore for JsonFileStore {
    async micro save_page(mut self, url: &str, data: &CrawledData) -> Result<(), StoreError> {
        let json_data = serde_json::to_string(data)?
        writeln!(self.writer, "{}", json_data)?
        self.writer.flush()?
        Ok(())
    }
    
    async micro save_links(mut self, parent_url: &str, links: &[String]) -> Result<(), StoreError> {
        let link_data = serde_json::json!({
            "parent_url": parent_url,
            "links": links,
            "discovered_at": SystemTime::now()
        })
        
        writeln!(self.writer, "{}", link_data)?
        self.writer.flush()?
        Ok(())
    }
    
    async micro get_crawled_urls(self) -> Result<HashSet<String>, StoreError> {
        # ä»æ–‡ä»¶è¯»å–å·²çˆ¬å–çš„ URL
        let content = std::fs::read_to_string(&self.file_path)?
        let mut urls = HashSet::new()
        
        for line in content.lines() {
            if let Ok(data) = serde_json::from_str::<CrawledData>(line) {
                urls.insert(data.url)
            }
        }
        
        Ok(urls)
    }
}

enum StoreError {
    DatabaseError(sqlx::Error),
    IoError(std::io::Error),
    SerializationError(serde_json::Error)
}
```

### å®Œæ•´çš„çˆ¬è™«å¼•æ“

```valkyrie
use std::sync::Arc
use tokio::sync::Semaphore

# ä¸»çˆ¬è™«å¼•æ“
class WebCrawler<S: DataStore> {
    client: AsyncHttpClient,
    scheduler: CrawlerScheduler,
    parser: HtmlParser,
    store: S,
    semaphore: Arc<Semaphore>,
    config: CrawlerEngineConfig
}

struct CrawlerEngineConfig {
    max_depth: u32,
    max_pages: Option<usize>,
    allowed_domains: Option<HashSet<String>>,
    url_filters: Vec<Regex>,
    content_filters: Vec<ContentFilter>
}

struct ContentFilter {
    selector: String,
    min_length: Option<usize>,
    required_keywords: Vec<String>
}

impl<S: DataStore> WebCrawler<S> {
    micro new(config: CrawlerConfig, store: S) -> WebCrawler<S> {
        let client = AsyncHttpClient::new(config.clone()).await
        let scheduler = CrawlerScheduler::new(config.requests_per_second)
        
        WebCrawler {
            client,
            scheduler,
            parser: HtmlParser::new(String::new()),
            store,
            semaphore: Arc::new(Semaphore::new(config.max_concurrent)),
            config: CrawlerEngineConfig::default()
        }
    }
    
    async micro crawl(mut self, seed_urls: Vec<String>) -> Result<CrawlStats, CrawlerError> {
        let mut stats = CrawlStats::new()
        
        # æ·»åŠ ç§å­ URL
        for url in seed_urls {
            self.scheduler.queue.lock().unwrap().add_url(url, Priority::High, 0)
        }
        
        # è·å–å·²çˆ¬å–çš„ URL
        let crawled_urls = self.store.get_crawled_urls().await?
        
        # ä¸»çˆ¬å–å¾ªç¯
        while let Some(task) = self.scheduler.schedule_request().await {
            if crawled_urls.contains(&task.url) {
                continue
            }
            
            if let Some(max_pages) = self.config.max_pages {
                if stats.pages_crawled >= max_pages {
                    break
                }
            }
            
            # è·å–å¹¶å‘è®¸å¯
            let permit = self.semaphore.clone().acquire_owned().await.unwrap()
            
            let client = self.client.clone()
            let store = self.store.clone()
            let parser = self.parser.clone()
            let scheduler = self.scheduler.clone()
            let config = self.config.clone()
            
            # å¼‚æ­¥å¤„ç†å•ä¸ªé¡µé¢
            tokio::spawn(async move {
                let _permit = permit  # ç¡®ä¿è®¸å¯åœ¨ä»»åŠ¡ç»“æŸæ—¶é‡Šæ”¾
                
                match Self::crawl_single_page(client, store, parser, task, config).await {
                    Ok(page_data) => {
                        # æ·»åŠ å‘ç°çš„é“¾æ¥åˆ°é˜Ÿåˆ—
                        for link in &page_data.links {
                            if page_data.depth < config.max_depth {
                                scheduler.queue.lock().unwrap().add_url(
                                    link.clone(), 
                                    Priority::Normal, 
                                    page_data.depth + 1
                                )
                            }
                        }
                        
                        scheduler.queue.lock().unwrap().mark_completed(&page_data.url)
                    },
                    Err(e) => {
                        eprintln!("Failed to crawl {}: {:?}", task.url, e)
                        scheduler.queue.lock().unwrap().mark_failed(&task.url)
                    }
                }
            })
            
            stats.pages_crawled += 1  # é¡µé¢è®¡æ•° Î£++
        }
        
        Ok(stats)
    }
    
    async micro crawl_single_page(
        client: AsyncHttpClient,
        mut store: S,
        parser: HtmlParser,
        task: CrawlTask,
        config: CrawlerEngineConfig
    ) -> Result<CrawledPageData, CrawlerError> {
        let start_time = SystemTime::now()
        
        # å‘é€ HTTP è¯·æ±‚
        let response = client.get(&task.url).await?
        let response_time = SystemTime::now().duration_since(start_time).unwrap_or_default()
        
        # æ£€æŸ¥å†…å®¹ç±»å‹
        let content_type = response.headers()
            .get("content-type")
            .and_then(|v| v.to_str().ok())
            .unwrap_or("")
        
        if !content_type.contains("text/html") {
            return Err(CrawlerError::UnsupportedContentType(content_type.to_string()))
        }
        
        # è§£æ HTML
        let html = response.text().await?
        let doc = parser.parse(&html)?
        
        # æå–æ•°æ®
        let links = parser.extract_links(&doc)
        let structured_data = parser.extract_structured_data(&doc)
        let content = parser.extract_text(&doc, "body").join("\n")
        
        # åº”ç”¨å†…å®¹è¿‡æ»¤å™¨
        if !Self::passes_content_filters(&content, &config.content_filters) {
            return Err(CrawlerError::ContentFiltered)
        }
        
        # è¿‡æ»¤é“¾æ¥
        let filtered_links = Self::filter_links(&links, &config)
        
        let crawled_data = CrawledData {
            url: task.url.clone(),
            title: structured_data.title.clone(),
            content,
            links: filtered_links.clone(),
            metadata: structured_data,
            crawled_at: SystemTime::now(),
            response_time,
            status_code: response.status().as_u16()
        }
        
        # ä¿å­˜æ•°æ®
        store.save_page(&task.url, &crawled_data).await?
        store.save_links(&task.url, &filtered_links).await?
        
        Ok(CrawledPageData {
            url: task.url,
            links: filtered_links,
            depth: task.depth
        })
    }
    
    micro filter_links(links: &[String], config: &CrawlerEngineConfig) -> Vec<String> {
        links.iter()
            .filter(|link| {
                # åŸŸåè¿‡æ»¤
                if let Some(allowed_domains) = &config.allowed_domains {
                    if let Some(domain) = extract_domain(link) {
                        if !allowed_domains.contains(&domain) {
                            return false
                        }
                    }
                }
                
                # URL æ¨¡å¼è¿‡æ»¤
                for filter in &config.url_filters {
                    if filter.is_match(link) {
                        return false
                    }
                }
                
                true
            })
            .cloned()
            .collect()
    }
    
    micro passes_content_filters(content: &str, filters: &[ContentFilter]) -> bool {
        for filter in filters {
            if let Some(min_length) = filter.min_length {
                if content.len() < min_length {
                    return false
                }
            }
            
            for keyword in &filter.required_keywords {
                if !content.to_lowercase().contains(&keyword.to_lowercase()) {
                    return false
                }
            }
        }
        
        true
    }
}

struct CrawledPageData {
    url: String,
    links: Vec<String>,
    depth: u32
}

struct CrawlStats {
    pages_crawled: usize,
    links_discovered: usize,
    errors: usize,
    start_time: SystemTime
}

impl CrawlStats {
    micro new() -> CrawlStats {
        CrawlStats {
            pages_crawled: 0,
            links_discovered: 0,
            errors: 0,
            start_time: SystemTime::now()
        }
    }
}
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€çˆ¬è™«

```valkyrie
async micro basic_crawler_example() -> Result<(), Box<dyn std::error::Error>> {
    # é…ç½®çˆ¬è™«
    let config = CrawlerConfig {
        max_concurrent: 10,
        request_delay: Duration::from_millis(100),
        timeout: Duration::from_secs(30),
        user_agent: "Valkyrie-Crawler/1.0".to_string(),
        max_retries: 3
    }
    
    # åˆ›å»ºæ•°æ®å­˜å‚¨
    let store = JsonFileStore::new("crawled_data.jsonl".to_string())?
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    let mut crawler = WebCrawler::new(config, store)
    
    # è®¾ç½®çˆ¬è™«å¼•æ“é…ç½®
    crawler.config = CrawlerEngineConfig {
        max_depth: 3,
        max_pages: Some(1000),
        allowed_domains: Some(hashset!["example.com".to_string()]),
        url_filters: vec![
            Regex::new(r"\.(jpg|jpeg|png|gif|pdf)$")?,
            Regex::new(r"/admin/")?
        ],
        content_filters: vec![
            ContentFilter {
                selector: "body".to_string(),
                min_length: Some(100),
                required_keywords: vec![]
            }
        ]
    }
    
    # å¼€å§‹çˆ¬å–
    let seed_urls = vec![
        "https://example.com".to_string(),
        "https://example.com/blog".to_string()
    ]
    
    let stats = crawler.crawl(seed_urls).await?
    
    println!("çˆ¬å–å®Œæˆ:")
    println!("  é¡µé¢æ•°: {}", stats.pages_crawled)
    println!("  é“¾æ¥æ•°: {}", stats.links_discovered)
    println!("  é”™è¯¯æ•°: {}", stats.errors)
    
    Ok(())
}
```

### ç”µå•†æ•°æ®çˆ¬å–

```valkyrie
struct ProductData {
    name: String,
    price: Option<f64>,
    description: String,
    images: Vec<String>,
    specifications: HashMap<String, String>,
    reviews_count: Option<u32>,
    rating: Option<f32>
}

class EcommerceCrawler {
    base_crawler: WebCrawler<DatabaseStore>,
    product_selectors: ProductSelectors
}

struct ProductSelectors {
    name: String,
    price: String,
    description: String,
    images: String,
    specifications: String,
    reviews: String
}

impl EcommerceCrawler {
    async micro crawl_products(mut self, category_urls: Vec<String>) -> Result<Vec<ProductData>, CrawlerError> {
        let mut products = Vec::new()
        
        for category_url in category_urls {
            let response = self.base_crawler.client.get(&category_url).await?
            let html = response.text().await?
            let doc = self.base_crawler.parser.parse(&html)?
            
            # æå–äº§å“é“¾æ¥
            let product_links = self.extract_product_links(&doc)
            
            # çˆ¬å–æ¯ä¸ªäº§å“é¡µé¢
            for product_url in product_links {
                if let Ok(product) = self.crawl_single_product(&product_url).await {
                    products.push(product)
                }
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                tokio::time::sleep(Duration::from_millis(500)).await
            }
        }
        
        Ok(products)
    }
    
    async micro crawl_single_product(self, url: &str) -> Result<ProductData, CrawlerError> {
        let response = self.base_crawler.client.get(url).await?
        let html = response.text().await?
        let doc = self.base_crawler.parser.parse(&html)?
        
        let name = self.extract_product_name(&doc)
        let price = self.extract_product_price(&doc)
        let description = self.extract_product_description(&doc)
        let images = self.extract_product_images(&doc)
        let specifications = self.extract_specifications(&doc)
        let (reviews_count, rating) = self.extract_reviews_info(&doc)
        
        Ok(ProductData {
            name,
            price,
            description,
            images,
            specifications,
            reviews_count,
            rating
        })
    }
}
```

### æ–°é—»èšåˆçˆ¬è™«

```valkyrie
struct NewsArticle {
    title: String,
    content: String,
    author: Option<String>,
    published_at: Option<SystemTime>,
    source: String,
    category: Option<String>,
    tags: Vec<String>,
    url: String
}

class NewsCrawler {
    base_crawler: WebCrawler<DatabaseStore>,
    news_sources: Vec<NewsSource>
}

struct NewsSource {
    name: String,
    base_url: String,
    article_selector: String,
    title_selector: String,
    content_selector: String,
    author_selector: String,
    date_selector: String,
    date_format: String
}

impl NewsCrawler {
    async micro crawl_latest_news(mut self) -> Result<Vec<NewsArticle>, CrawlerError> {
        let mut articles = Vec::new()
        
        for source in &self.news_sources {
            match self.crawl_news_source(source).await {
                Ok(mut source_articles) => articles.append(&mut source_articles),
                Err(e) => eprintln!("Failed to crawl {}: {:?}", source.name, e)
            }
        }
        
        # å»é‡å’Œæ’åº
        articles.sort_by(|a, b| b.published_at.cmp(&a.published_at))
        articles.dedup_by(|a, b| a.title == b.title)
        
        Ok(articles)
    }
    
    async micro crawl_news_source(self, source: &NewsSource) -> Result<Vec<NewsArticle>, CrawlerError> {
        let response = self.base_crawler.client.get(&source.base_url).await?
        let html = response.text().await?
        let doc = self.base_crawler.parser.parse(&html)?
        
        let article_links = self.extract_article_links(&doc, &source.article_selector)
        let mut articles = Vec::new()
        
        for link in article_links {
            if let Ok(article) = self.crawl_single_article(&link, source).await {
                articles.push(article)
            }
        }
        
        Ok(articles)
    }
}
```

## é«˜çº§ç‰¹æ€§

### åˆ†å¸ƒå¼çˆ¬å–

```valkyrie
use std::redis::{RedisClient, RedisConnection}

# åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
class DistributedCrawler {
    redis: RedisConnection,
    worker_id: String,
    local_crawler: WebCrawler<DatabaseStore>
}

impl DistributedCrawler {
    async micro run_worker(mut self) -> Result<(), CrawlerError> {
        loop {
            # ä» Redis è·å–ä»»åŠ¡
            if let Some(task_json) = self.redis.blpop("crawl_queue", 10).await? {
                let task: CrawlTask = serde_json::from_str(&task_json)?
                
                # å¤„ç†ä»»åŠ¡
                match self.process_task(task).await {
                    Ok(result) => {
                        # å°†ç»“æœæ¨é€åˆ°ç»“æœé˜Ÿåˆ—
                        let result_json = serde_json::to_string(&result)?
                        self.redis.lpush("crawl_results", result_json).await?
                    },
                    Err(e) => {
                        # é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
                        eprintln!("Task failed: {:?}", e)
                    }
                }
            }
        }
    }
    
    async micro add_urls_to_queue(mut self, urls: Vec<String>) -> Result<(), CrawlerError> {
        for url in urls {
            let task = CrawlTask {
                url,
                depth: 0,
                priority: Priority::Normal,
                metadata: TaskMetadata::default()
            }
            
            let task_json = serde_json::to_string(&task)?
            self.redis.lpush("crawl_queue", task_json).await?
        }
        
        Ok(())
    }
}
```

### æ™ºèƒ½åçˆ¬è™«

```valkyrie
# åçˆ¬è™«æ£€æµ‹å’Œè§„é¿
class AntiDetectionCrawler {
    user_agents: Vec<String>,
    proxy_pool: Vec<String>,
    session_manager: SessionManager
}

struct SessionManager {
    sessions: HashMap<String, CrawlSession>,
    max_requests_per_session: u32
}

struct CrawlSession {
    cookies: HashMap<String, String>,
    headers: HashMap<String, String>,
    request_count: u32,
    created_at: SystemTime
}

impl AntiDetectionCrawler {
    async micro crawl_with_evasion(mut self, url: &str) -> Result<Response, CrawlerError> {
        # éšæœºé€‰æ‹© User-Agent
        let user_agent = self.user_agents.choose(&mut rand::thread_rng()).unwrap()
        
        # è·å–æˆ–åˆ›å»ºä¼šè¯
        let session = self.session_manager.get_or_create_session(url)
        
        # æ„å»ºè¯·æ±‚
        let mut request = Request::builder()
            .uri(url)
            .header("User-Agent", user_agent)
            .header("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
            .header("Accept-Language", "en-US,en;q=0.5")
            .header("Accept-Encoding", "gzip, deflate")
            .header("Connection", "keep-alive")
            .header("Upgrade-Insecure-Requests", "1")
        
        # æ·»åŠ ä¼šè¯ cookies
        if !session.cookies.is_empty() {
            let cookie_header = session.cookies.iter()
                .map(|(k, v)| format!("{}={}", k, v))
                .collect::<Vec<_>>()
                .join("; ")
            request = request.header("Cookie", cookie_header)
        }
        
        # æ·»åŠ éšæœºå»¶è¿Ÿ
        let delay = rand::thread_rng().gen_range(1000..5000)
        tokio::time::sleep(Duration::from_millis(delay)).await
        
        # å‘é€è¯·æ±‚
        let response = self.send_request(request.build()?).await?
        
        # æ›´æ–°ä¼šè¯
        self.session_manager.update_session(url, &response)
        
        Ok(response)
    }
}
```

Valkyrie çš„ç½‘ç»œçˆ¬è™«æ¡†æ¶æä¾›äº†ä»ç®€å•æ•°æ®é‡‡é›†åˆ°å¤§è§„æ¨¡åˆ†å¸ƒå¼çˆ¬å–çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é«˜æ€§èƒ½ã€å¯æ‰©å±•ã€æ˜“ç»´æŠ¤çš„ç‰¹ç‚¹ï¼Œé€‚ç”¨äºå„ç§æ•°æ®é‡‡é›†åœºæ™¯ã€‚