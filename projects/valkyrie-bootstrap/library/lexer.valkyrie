# Valkyrie 词法分析器
# 使用 Valkyrie 语言实现

# Token 类型
let Token = {
    type = "",
    value = "",
    line = 0,
    column = 0,
}

# TokenType 枚举
let TokenType = {
    # 关键字
    LET = "LET",
    MICRO = "MICRO", 
    IF = "IF",
    ELSE = "ELSE",
    
    # 标识符和字面量
    IDENTIFIER = "IDENTIFIER",
    NUMBER = "NUMBER", 
    STRING = "STRING",
    BOOLEAN = "BOOLEAN",
    
    # 操作符
    ASSIGN = "ASSIGN",        # =
    PLUS = "PLUS",            # +
    MINUS = "MINUS",          # -
    MULTIPLY = "MULTIPLY",    # *
    DIVIDE = "DIVIDE",        # /
    EQUAL = "EQUAL",          # ==
    NOT_EQUAL = "NOT_EQUAL",  # !=
    LESS = "LESS",            # <
    GREATER = "GREATER",      # >
    OR = "OR",                # ||
    AND = "AND",              # &&
    
    # 符号
    LPAREN = "LPAREN",        # (
    RPAREN = "RPAREN",        # )
    LBRACE = "LBRACE",        # {
    RBRACE = "RBRACE",        # }
    LBRACKET = "LBRACKET",    # [
    RBRACKET = "RBRACKET",    # ]
    COMMA = "COMMA",          # ,
    SEMICOLON = "SEMICOLON",  # ;
    COLON = "COLON",          # :
    ARROW = "ARROW",          # ->
    
    # 特殊
    EOF = "EOF",
    COMMENT = "COMMENT",
}

# 词法分析器类
let Lexer = {
    source = "",
    position = 0,
    line = 1,
    column = 1,
    tokens = [],
    keywords = {},
}

# 初始化 Lexer
micro initLexer(source) {
    let lexer = {}
    lexer.source = source
    lexer.position = 0
    lexer.line = 1
    lexer.column = 1
    lexer.tokens = []
    lexer.keywords = {}
    lexer
}

# 获取当前字符
micro current(lexer) {
    if lexer.position >= lexer.source.length { 
        "" 
    } else { 
        lexer.source[lexer.position] 
    }
}

# 向前查看字符
micro peek(lexer, offset) {
    let pos = lexer.position + offset
    if pos >= lexer.source.length {
        ""
    } else {
        lexer.source[pos]
    }
}

# 前进一个字符
micro advance(lexer) {
    if lexer.position < lexer.source.length {
        if lexer.source[lexer.position] == "\n" {
            lexer.line = lexer.line + 1
            lexer.column = 1
        } else {
            lexer.column = lexer.column + 1
        }
        lexer.position = lexer.position + 1
    }
}

# 跳过空白字符
micro skipWhitespace(lexer) {
    let char = current(lexer)
    if (char == " ") || (char == "\t") || (char == "\r") || (char == "\n") {
        advance(lexer)
        skipWhitespace(lexer)
    } else {
        # 不是空白字符，直接返回
        # 不需要返回值，这是一个递归函数的终止条件
    }
}

# 读取字符串
micro readString(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    advance(lexer) # 跳过开始的引号
    
    let value = ""
    let char = current(lexer)
    if char != "" && char != "\"" {
        if char == "\\" {
            advance(lexer)
            let escaped = current(lexer)
            if escaped == "n" {
                value = value + "\n"
            } else if escaped == "t" {
                value = value + "\t"
            } else if escaped == "r" {
                value = value + "\r"
            } else if escaped == "\\" {
                value = value + "\\"
            } else if escaped == "\"" {
                value = value + "\""
            } else {
                value = value + escaped
            }
        } else {
            value = value + char
        }
        advance(lexer)
        let nextValue = readString(lexer)
        value = value + nextValue.value
    }
    
    advance(lexer) # 跳过结束的引号
    
    let token = {}
    token.type = TokenType.STRING
    token.value = value
    token.line = startLine
    token.column = startColumn
    token
}

# 读取数字
micro readNumber(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    let value = ""
    
    let char = current(lexer)
    if char >= "0" && char <= "9" {
        value = value + char
        advance(lexer)
        let nextValue = readNumber(lexer)
        value = value + nextValue.value
    }
    
    # 处理小数点
    if current(lexer) == "." {
        value = value + "."
        advance(lexer)
        let char2 = current(lexer)
        if char2 >= "0" && char2 <= "9" {
            value = value + char2
            advance(lexer)
            let nextValue2 = readNumber(lexer)
            value = value + nextValue2.value
        }
    }
    
    let token = {}
    token.type = TokenType.NUMBER
    token.value = value
    token.line = startLine
    token.column = startColumn
    token
}

# 读取标识符
micro readIdentifier(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    let value = ""
    
    let char = current(lexer)
    if (char >= "a" && char <= "z") || (char >= "A" && char <= "Z") || char == "_" || (char >= "0" && char <= "9") {
        value = value + char
        advance(lexer)
        let nextValue = readIdentifier(lexer)
        value = value + nextValue.value
    }
    
    let tokenType = lexer.keywords[value]
    if tokenType == "" {
        tokenType = TokenType.IDENTIFIER
    }
    
    let tokenValue = value
    if tokenType == TokenType.BOOLEAN {
        if value == "true" {
            tokenValue = true
        } else {
            tokenValue = false
        }
    }
    
    let token = {}
    token.type = tokenType
    token.value = tokenValue
    token.line = startLine
    token.column = startColumn
    token
}

# 读取注释
micro readComment(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    advance(lexer) # 跳过 #
    
    let value = ""
    let char = current(lexer)
    if char != "" && char != "\n" {
        value = value + char
        advance(lexer)
        let nextValue = readComment(lexer)
        value = value + nextValue.value
    }
    
    let token = {}
    token.type = TokenType.COMMENT
    token.value = value
    token.line = startLine
    token.column = startColumn
    token
}

# 创建 Token
micro createToken(type, value, line, column) {
    let token = {}
    token.type = type
    token.value = value
    token.line = line
    token.column = column
    token
}

# 词法分析主函数
micro tokenize(lexer) {
    if lexer.position < lexer.source.length {
        skipWhitespace(lexer)
        
        let char = current(lexer)
        if char != "" {
            let line = lexer.line
            let column = lexer.column
            
            # 注释
            if char == "#" {
                let comment = readComment(lexer)
                lexer.tokens.push(comment)
                tokenize(lexer)
            } else if char == "\"" {
                # 字符串
                let str = readString(lexer)
                lexer.tokens.push(str)
                tokenize(lexer)
            } else if char >= "0" && char <= "9" {
                # 数字
                let num = readNumber(lexer)
                lexer.tokens.push(num)
                tokenize(lexer)
            } else if (char >= "a" && char <= "z") || (char >= "A" && char <= "Z") || char == "_" {
                # 标识符和关键字
                let id = readIdentifier(lexer)
                lexer.tokens.push(id)
                tokenize(lexer)
            } else if char == "=" && peek(lexer, 1) == "=" {
                # ==
                let token = createToken(TokenType.EQUAL, "==", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
                tokenize(lexer)
            } else if char == "!" && peek(lexer, 1) == "=" {
                # !=
                let token = createToken(TokenType.NOT_EQUAL, "!=", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
                tokenize(lexer)
            } else if char == "|" && peek(lexer, 1) == "|" {
                # ||
                let token = createToken(TokenType.OR, "||", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
                tokenize(lexer)
            } else if char == "&" && peek(lexer, 1) == "&" {
                # &&
                let token = createToken(TokenType.AND, "&&", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
                tokenize(lexer)
            } else if char == "-" && peek(lexer, 1) == ">" {
                # ->
                let token = createToken(TokenType.ARROW, "->", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
                tokenize(lexer)
            } else if char == "=" {
                let token = createToken(TokenType.ASSIGN, "=", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "+" {
                let token = createToken(TokenType.PLUS, "+", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "-" {
                let token = createToken(TokenType.MINUS, "-", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "*" {
                let token = createToken(TokenType.MULTIPLY, "*", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "/" {
                let token = createToken(TokenType.DIVIDE, "/", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "<" {
                let token = createToken(TokenType.LESS, "<", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == ">" {
                let token = createToken(TokenType.GREATER, ">", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "(" {
                let token = createToken(TokenType.LPAREN, "(", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == ")" {
                let token = createToken(TokenType.RPAREN, ")", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "{" {
                let token = createToken(TokenType.LBRACE, "{", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "}" {
                let token = createToken(TokenType.RBRACE, "}", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "[" {
                let token = createToken(TokenType.LBRACKET, "[", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "]" {
                let token = createToken(TokenType.RBRACKET, "]", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == "," {
                let token = createToken(TokenType.COMMA, ",", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == ";" {
                let token = createToken(TokenType.SEMICOLON, ";", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else if char == ":" {
                let token = createToken(TokenType.COLON, ":", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                tokenize(lexer)
            } else {
                advance(lexer)
                tokenize(lexer)
            }
        }
    }
    
    # 添加 EOF token
    let eofToken = createToken(TokenType.EOF, "", lexer.line, lexer.column)
    lexer.tokens.push(eofToken)
    lexer.tokens
}