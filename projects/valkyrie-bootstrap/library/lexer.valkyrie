# namepath package.lexer
# Valkyrie 词法分析器
# 使用 Valkyrie 语言实现

# Token 类型
# 词法分析器
# 定义 Token 结构

let Token = {
    type = "",
    value = "",
    line = 0,
    column = 0
}

# TokenType 枚举
let TokenType = {
    LET = "LET",
    MICRO = "MICRO", 
    EXPORT = "EXPORT",
    IF = "IF",
    ELSE = "ELSE",
    WHILE = "WHILE",
    
    IDENTIFIER = "IDENTIFIER",
    NUMBER = "NUMBER",
    STRING = "STRING",
    BOOLEAN = "BOOLEAN",
    
    ASSIGN = "ASSIGN",
    PLUS = "PLUS",
    MINUS = "MINUS",
    MULTIPLY = "MULTIPLY",
    DIVIDE = "DIVIDE",
    EQUAL = "EQUAL",
    NOT_EQUAL = "NOT_EQUAL",
    LESS = "LESS",
    GREATER = "GREATER",
    LESS_EQUAL = "LESS_EQUAL",
    GREATER_EQUAL = "GREATER_EQUAL",
    OR = "OR",
    AND = "AND",
    
    LPAREN = "LPAREN",
    RPAREN = "RPAREN",
    LBRACE = "LBRACE",
    RBRACE = "RBRACE",
    LBRACKET = "LBRACKET",
    RBRACKET = "RBRACKET",
    COMMA = "COMMA",
    SEMICOLON = "SEMICOLON",
    COLON = "COLON",
    DOT = "DOT",
    ARROW = "ARROW",
    
    EOF = "EOF",
    COMMENT = "COMMENT",
}

# 初始化 Lexer
micro initLexer(source) {
    let lexer = {}
    lexer.source = source
    lexer.position = 0
    lexer.line = 1
    lexer.column = 1
    lexer.tokens = []
    lexer.keywords = {}
    lexer.keywords["let"] = "LET"
    lexer.keywords["micro"] = "MICRO"
    lexer.keywords["export"] = "EXPORT"
    lexer.keywords["if"] = "IF"
    lexer.keywords["else"] = "ELSE"
    lexer.keywords["while"] = "WHILE"
    lexer.keywords["true"] = "TRUE"
    lexer.keywords["false"] = "FALSE"
    return lexer
}

# 获取当前字符
micro current(lexer) {
    if lexer.position >= lexer.source.length { 
        "" 
    } else { 
        lexer.source[lexer.position] 
    }
}

# 向前查看字符
micro peek(lexer, offset) {
    let pos = lexer.position + offset
    if pos >= lexer.source.length {
        ""
    } else {
        lexer.source[pos]
    }
}

# 前进一个字符
micro advance(lexer) {
    if lexer.position < lexer.source.length {
        if lexer.source[lexer.position] == "\n" {
            lexer.line = lexer.line + 1
            lexer.column = 1
        } else {
            lexer.column = lexer.column + 1
        }
        lexer.position = lexer.position + 1
    }
}

# 跳过空白字符
micro skipWhitespace(lexer) {
    let char = current(lexer)
    while (char == " ") || (char == "\t") || (char == "\r") || (char == "\n") {
        advance(lexer)
        char = current(lexer)
    }
}

# 读取字符串
micro readString(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    advance(lexer) # 跳过开始的引号
    
    let value = ""
    while current(lexer) != "\"" && current(lexer) != "" {
        let char = current(lexer)
        if char == "\\" {
            advance(lexer)
            let escaped = current(lexer)
            if escaped == "n" {
                value = value + "\n"
            } else if escaped == "t" {
                value = value + "\t"
            } else if escaped == "r" {
                value = value + "\r"
            } else if escaped == "\\" {
                value = value + "\\"
            } else if escaped == "\"" {
                value = value + "\""
            } else {
                value = value + escaped
            }
        } else {
            value = value + char
        }
        advance(lexer)
    }
    
    advance(lexer) # 跳过结束的引号
    
    let token = {}
    token.type = "STRING"
    token.value = value
    token.line = startLine
    token.column = startColumn
    token
}

# 读取数字
micro readNumber(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    let value = ""
    
    while current(lexer) >= "0" && current(lexer) <= "9" {
        value = value + current(lexer)
        advance(lexer)
    }
    
    if current(lexer) == "." {
        value = value + "."
        advance(lexer)
        
        while current(lexer) >= "0" && current(lexer) <= "9" {
            value = value + current(lexer)
            advance(lexer)
        }
    }
    
    let token = {}
    token.type = "NUMBER"
    token.value = value
    token.line = startLine
    token.column = startColumn
    token
}

# 读取标识符
micro readIdentifier(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    let value = ""
    
    let char = current(lexer)
    while (char >= "a" && char <= "z") || (char >= "A" && char <= "Z") || char == "_" || (char >= "0" && char <= "9") {
        value = value + char
        advance(lexer)
        char = current(lexer)
    }
    
    let tokenType = lexer.keywords[value]
    if tokenType == undefined {
        tokenType = "IDENTIFIER"
    }
    
    # 不需要特殊处理布尔值，直接使用原始值
    let tokenValue = value
    
    let token = {}
    token.type = tokenType
    token.value = tokenValue
    token.line = startLine
    token.column = startColumn
    token
}

# 读取注释
micro readComment(lexer) {
    let startLine = lexer.line
    let startColumn = lexer.column
    advance(lexer) # 跳过 #
    
    let value = ""
    while current(lexer) != "" && current(lexer) != "\n" {
        value = value + current(lexer)
        advance(lexer)
    }
    
    let token = {}
    token.type = "COMMENT"
    token.value = value
    token.line = startLine
    token.column = startColumn
    token
}

# 创建 Token
micro createToken(type, value, line, column) {
    let token = {}
    token.type = type
    token.value = value
    token.line = line
    token.column = column
    token
}

# 词法分析主函数
micro tokenize(lexer) {
    while lexer.position < lexer.source.length {
        skipWhitespace(lexer)
        
        let char = current(lexer)
        if char == "" {
            # End of source, break loop by doing nothing
        } else {
            let line = lexer.line
            let column = lexer.column
            
            # 注释
            if char == "#" {
                let comment = readComment(lexer)
                lexer.tokens.push(comment)
            } else if char == "\"" {
                # 字符串
                let str = readString(lexer)
                lexer.tokens.push(str)
            } else if char >= "0" && char <= "9" {
                # 数字
                let num = readNumber(lexer)
                lexer.tokens.push(num)
            } else if (char >= "a" && char <= "z") || (char >= "A" && char <= "Z") || char == "_" {
                # 标识符和关键字
                let id = readIdentifier(lexer)
                lexer.tokens.push(id)
            } else if char == "=" && peek(lexer, 1) == "=" {
                # ==
                let token = createToken("EQUAL", "==", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
            } else if char == "!" && peek(lexer, 1) == "=" {
                # !=
                let token = createToken("NOT_EQUAL", "!=", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
            } else if char == "|" && peek(lexer, 1) == "|" {
                # ||
                let token = createToken("OR", "||", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
            } else if char == "&" && peek(lexer, 1) == "&" {
                # &&
                let token = createToken("AND", "&&", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
            } else if char == "-" && peek(lexer, 1) == ">" {
                # ->
                let token = createToken("ARROW", "->", line, column)
                lexer.tokens.push(token)
                advance(lexer)
                advance(lexer)
            } else if char == "=" {
                let token = createToken("ASSIGN", "=", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "+" {
                let token = createToken("PLUS", "+", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "-" {
                let token = createToken("MINUS", "-", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "*" {
                let token = createToken("MULTIPLY", "*", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "/" {
                let token = createToken("DIVIDE", "/", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "<" {
                if peek(lexer, 1) == "=" {
                    let token = createToken("LESS_EQUAL", "<=", line, column)
                    lexer.tokens.push(token)
                    advance(lexer)
                    advance(lexer)
                } else {
                    let token = createToken("LESS", "<", line, column)
                    lexer.tokens.push(token)
                    advance(lexer)
                }
            } else if char == ">" {
                if peek(lexer, 1) == "=" {
                    let token = createToken("GREATER_EQUAL", ">=", line, column)
                    lexer.tokens.push(token)
                    advance(lexer)
                    advance(lexer)
                } else {
                    let token = createToken("GREATER", ">", line, column)
                    lexer.tokens.push(token)
                    advance(lexer)
                }
            } else if char == "(" {
                let token = createToken("LPAREN", "(", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == ")" {
                let token = createToken("RPAREN", ")", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "{" {
                let token = createToken("LBRACE", "{", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "}" {
                let token = createToken("RBRACE", "}", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "[" {
                let token = createToken("LBRACKET", "[", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "]" {
                let token = createToken("RBRACKET", "]", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "," {
                let token = createToken("COMMA", ",", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == ";" {
                let token = createToken("SEMICOLON", ";", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == ":" {
                let token = createToken("COLON", ":", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else if char == "." {
                let token = createToken("DOT", ".", line, column)
                lexer.tokens.push(token)
                advance(lexer)
            } else {
                # 遇到未知字符，跳过
                advance(lexer)
            }
        }
    }
    
    # 添加 EOF token
    let eofToken = createToken("EOF", "", lexer.line, lexer.column)
    lexer.tokens.push(eofToken)
    lexer.tokens
}

# 导出函数
export {initLexer, tokenize, TokenType}